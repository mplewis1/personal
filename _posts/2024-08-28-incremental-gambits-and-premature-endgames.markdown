---
layout: post
title:  "Incremental Gambits and Premature Endgames"
date:   2024-08-28
categories: blog
---

In chess, there are three phases to the game, all with distinct strategies: the opening, the middlegame, and the endgame. In the opening, the first 15 or so moves of the game, the goal is to deploy your pieces from their starting positions and control the center of the board. Because the starting position is always the same, a few schools and theories have emerged on the optimal approach to the opening. The middlegame involves back-and-forth offense and defense as players target each other's pieces via tactics and attempt to checkmate the king. The endgame begins when the majority of pieces have been captured without a checkmate. Only a few pieces remain at this point and a slower, mathematical trudge begins to checkmate the king and declare victory.

There are plenty of cliche parallels to draw between chess and business, but nowhere do I find them more apparent than in the current AI race. The progression has been:

- **Opening (2017-2022):** Researchers and companies developed models based on the transformer architecture, setting the pieces on the board.
- **Middlegame (2022-2024):** ChatGPT blew the center wide open. It sparked a flurry of innovation and competition, with companies rapidly building and deploying AI solutions.
- **Endgame (Today):** A handful of dominant players - chip designers, data center suppliers, hyperscalers, and top model houses - have consolidated power, working to maintain their leads and eliminate competition.

The status quo assumes that the transformer architecture will win. Companies up and down the stack have coalesced around transformers due to their generalizability and scalability. Consolidation has led to a research landscape dominated by incremental gambits - clever little hacks meant to squeeze more juice out of these models without fundamentally advancing their intelligence.

I argue that we've prematurely entered the endgame of the current AI wave, crowning a winning technology before fully exploring alternatives. This piece will examine the limitations of our current approach and propose alternatives that may lead us closer to achieving human-level general intelligence. As it stands, we risk a stalemate - trapped in a local optimum unable to reach AGI.

![Chess](/images/Chess.png)

### The Old King

[*Attention is All You Need*](https://arxiv.org/abs/1706.03762) introduced the [transformer](https://poloclub.github.io/transformer-explainer/) in 2017. Its [self-attention mechanism](https://www.sciencedirect.com/topics/computer-science/self-attention-mechanism#:~:text=A%20Self%2DAttention%20Mechanism%20is,when%20making%20predictions%20or%20decisions.) allows the model to more efficiently capture contextual relationships between words. In 2018, Google released [BERT](https://arxiv.org/abs/1810.04805) which introduced bi-directional pre-training. A massive, plain text dataset was used to train the model before it was fine-tuned to a specific task. This helped influence the development of OpenAI's GPT models which are trained on large corpora of text (and now other [multi-modal data](https://www.theverge.com/2024/4/6/24122915/openai-youtube-transcripts-gpt-4-training-data-google)), demonstrating impressive zero-shot learning capabilities. When ChatGPT was released in November 2022, the transformer went mainstream and started the race to build around it.

These early successes paved the way for the ubiquity of transformers across AI. The transformer's ability to [scale](https://arxiv.org/abs/2001.08361) effectively, capture long-range dependencies, and adapt to various language-based tasks has made them the go-to architecture across textual domains, including less obvious areas like [biology](https://www.nature.com/articles/s41592-024-02354-y) and [chemistry](https://arxiv.org/abs/2408.07246). Today, companies like OpenAI (GPT), Meta (Llama), Anthropic (Claude), xAI (Grok), and more are all-in on transformers and are pushing the boundaries of performance.

Despite the success, transformers feel like they're starting to reach their limits. While models incrementally improve with each refresh, it has become apparent that the path to AGI is much further away (if not [impossible](https://www.pcmag.com/news/meta-ai-chief-large-language-models-wont-achieve-agi)) using transformers alone. Performance gains in LLMs are becoming increasingly [difficult](https://dl.acm.org/doi/abs/10.1145/3531146.3533229) to achieve due to their insatiable [appetite](https://arxiv.org/abs/2404.04125) for training data and compute. The most advanced models [struggle](https://arxiv.org/abs/2210.09261) with complex reasoning tasks that require multi-step logical inference or abstract thinking. LLMs often [fail](https://arxiv.org/abs/2306.03341) to generalize knowledge in novel ways and instead rely on pattern-matching within their training data rather than demonstrating true cognitive understanding. All of this means that realistically, [superintelligence](https://situational-awareness.ai/) won't emerge out of the transformer architecture.

While transformers have completely upended consumer and enterprise workflows, they won't be the final step toward AGI (and I'd argue not even [SOTA within a few short years](https://www.isattentionallyouneed.com/)). The utopian hype following the release of ChatGPT has waned as expectations inflate. In the following sections, I'll explore these challenges, examining why transformers fall short in crucial areas like reasoning, generalization, and efficient scaling. We'll also look at alternative approaches that might address these fundamental issues and offer potential paths toward AGI.

### Incremental Gambits

At the heart of the transformer lies its fundamental challenge (ironically caused by its greatest strength): the [quadratic computational complexity](https://www.sciencedirect.com/topics/computer-science/quadratic-time-complexity#:~:text='Quadratic%20Time%20Complexity'%20refers%20to,techniques%20like%20RLTP%20and%20LTrP.) problem caused by the self-attention mechanism. This issue is a key reason why I believe the transformer won't be the architecture that achieves AGI.

In a transformer, each token in a sequence must attend to every other token, resulting in a computational complexity that grows quadratically with input length. In other words, a sequence of length n results in n^2 computations. This means that doubling the length of an input doesn't just double the computational requirements - it quadruples them. This scaling poses a problem for both training and inference, particularly as we aim to process longer sequences of text or incorporate multiple modalities. (I won't go into deeper detail about the mechanics, but you can find a great technical writeup [here](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)).

![Self-Attention](/images/self-attention.png)
[*Explainable AIL Visualizing Attention in Transformers (Comet)*](https://www.comet.com/site/blog/explainable-ai-for-transformers/)

The impact of quadratic complexity is evident in the limited context windows of current commercial models. For instance, despite having an [estimated](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/) 1.7B+ parameters, GPT-4o has a [context window of just 128K tokens](https://platform.openai.com/docs/models/gpt-4o) - equivalent to about 300 pages of text. Quadratic complexity has spawned a wave of research aimed at solving it.

A gambit in chess is when a player sacrifices a piece during the opening in exchange for material compensation later. They're tricks meant to do more with less. I call these recent improvements to manage quadratic complexity incremental gambits because they offer slight advantages over base transformers, but often introduce new trade-offs.

One popular approach has been the development of [sparse attention mechanisms](https://medium.com/@vishal09vns/sparse-attention-dad17691478c). Models like [Longformer](https://arxiv.org/abs/2004.05150) use patterns of sparse attention to reduce computational complexity by focusing on relationships between only a strategic subset of relevant tokens. While this allows for the efficient processing of much longer sequences, it can potentially miss important long-range dependencies in data. Another tactic has been the exploration of [linear attention mechanisms](https://arxiv.org/abs/2102.11174). These have shown that it's possible to approximate the attention computation with linear operations. In turn, computational requirements are significantly reduced, but so is the expressive power of the model. Researchers have also experimented with [recurrent memory techniques](https://arxiv.org/abs/2203.08913) to extend the effective context of transformers. These allow the model to access a large external memory, theoretically enabling indefinite context length. However, this approach also adds complexity without actually addressing the underlying transformer scaling issues. This [piece](https://www.mackenziemorehead.com/is-attention-all-you-need/) offers a more detailed look at attention alternatives.

While these approaches demonstrate ingenuity on the part of researchers working to push the boundaries of transformers, they ultimately fall short of addressing the fundamental issues with the architecture. Each approach outlined above essentially trades one limitation for another. They may extend context windows or reduce computational complexity, but often at the cost of model expressiveness and accuracy, or by introducing new caveats. More critically and discussed further in the following sections, these improvements do nothing to address the transformer's data and compute requirements nor the core inability to reason in a human-like manner. These gambits alone will not get us to AGI.

### Premature Endgames

[Chinchilla scaling laws](https://arxiv.org/abs/2203.15556) refined our understanding of the optimal balance between model size and training data. The DeepMind research team found that many LLMs were undertrained relative to their size, suggesting that making models bigger wasn't the most efficient path forward. This assertion starkly contrasted with the prevailing notions outlined in [Kaplan's scaling laws](https://arxiv.org/abs/2001.08361). As models grow, performance improvements become [increasingly unpredictable](https://arxiv.org/abs/2202.07785) and potentially disappointing. This explains then why larger models have been perceived as underwhelming.

The transition from Llama-3 70B to Llama-3.1 405B illustrated this challenge well. Despite a nearly 6x increase in parameters, Llama-3.1 405B only scored [3.2 points better](https://context.ai/compare/llama3-70b-instruct-v1/llama3-1-405b-instruct-v1) than Llama-3 70B on the 5-shot MMLU benchmark. This nominal improvement relative to the massive increase in model size, computational resources, and training time raises serious questions about the viability of continued scaling as a path to AGI.

From a purely economic perspective, the trend has translated into a staggering [capex problem](https://www.sequoiacap.com/article/ais-600b-question/) that analysts expect will continue to grow even with a [delayed Blackwell](https://www.theinformation.com/articles/nvidias-new-ai-chip-is-delayed-impacting-microsoft-google-meta) GPU. The AI arms race has led to unprecedented investment in compute infrastructure with companies betting big on the assumption that larger models will justify the costs. However, the ROI remains highly questionable.

I find that the private market and venture capital opinions of this are overly optimistic. Most arguments concede that there is a demand issue, but conclude by saying it will catch up and make this all worth it. I want to agree, but this is a classicly VC take that fails to capture the intricacies of the sub-problems or offer a compelling solution as to how we'll get there.

In a recent episode of [BG2](https://youtu.be/5LYZCoDysLs?si=ZZ9vG7QOF42hqrW9&t=1840), Brad Gerstner and Bill Gurley offered a more pragmatic view through the lens of public markets. 

There's a growing concern that supply is outpacing demand. While Jensen [estimates that $2T](https://www.datacenterdynamics.com/en/news/nvidia-ceo-jensen-huang-predicts-data-center-spend-will-double-to-2-trillion/) will be needed for the data center build-out by 2028 (with NVIDIA alone expecting to do $136B in data center revenue this year), it's unclear if end-user demand will materialize to justify these investments. At a [projected](https://x.com/modestproposal1/status/1828456896811909531) 14% of market-wide capital spending by 2026, NVIDIA is expected to be "comparable to IBM at the peak of the Mainframe Era or Cisco, Lucent, and Nortel at the peak of the New Economy Era." A foreboding interpretation of the tea leaves... (I write just hours before NVIDIA's Q2 earnings are released... hopefully this isn't stale already).

Even household AI applications are struggling to generate revenues commensurate with their hype. OpenAI expects to [bring in $3.4B in 2024](https://www.bloomberg.com/news/articles/2024-06-12/openai-doubles-annualized-revenue-to-3-4-billion-information), a fraction of what's needed to close the expanding air gap. There's a noticeable disconnect between growing user bases and actual engagement metrics, raising questions about the long-term value propositions of these offerings.

If you shoo away these objective criticisms, there's still the stifling reality that the capex-heavy landscape favors incumbents with giant balance sheets. Tech giants are moving fast to prevent independent AI companies from capturing market share. They are acutely aware that if private companies hit breakout potential, then unlimited capital will follow. That said, it's a lot easier to deploy from the war chest at will than to go out to raise additional, dilutive financing.

This has led to a wave of acquisitions of once-promising AI companies with world-class talent, including Google's acquisition of [Character.AI](https://fortune.com/2024/08/02/google-character-ai-founders-microsoft-inflection-amazon-adept/), Amazon's acquisition of [Adept](https://techcrunch.com/2024/06/28/amazon-hires-founders-away-from-ai-startup-adept/), and Microsoft's deal to buy [Inflection AI](https://techcommunity.microsoft.com/t5/us-canada-tech-talks-forum/microsoft-s-650-million-deal-with-inflection-ai-key-things-to/m-p/4095533). OpenAI itself is suffering an [exodus of top talent](https://fortune.com/2024/08/06/openai-sam-altman-executive-departures-tech-exodus/). Perhaps we should have considered the possibility it could be a [fourth-stage empire](https://www.davidmurrin.co.uk/article/5-phase-life-cycle) amidst the rumors of a [$7T fundraise](https://www.wsj.com/tech/ai/sam-altman-seeks-trillions-of-dollars-to-reshape-business-of-chips-and-ai-89ab3db0) back in February.

There's a misalignment between massive capex and uncertain future revenues. While big tech executives acknowledge that capex and revenues will never be perfectly aligned, it's concerning that Microsoft's CFO expects its quickly depreciating generative AI assets to [monetize over 15 years](https://www.forbes.com/sites/petercohan/2024/07/31/microsoft-stock-drops-as-ai-capital-expenditures-surge-to-56-billion/). We're in uncharted territory hoping that breakthrough applications like personal agents (which I don’t believe to be possible with current architecture) will eventually justify these investments.

On [Invest Like the Best](https://open.spotify.com/episode/05Fx7yNSEA148kHr1znbrb?si=6c5248ab1a5045e2), Gavin Baker painted a more concerning picture. He argues that the hyperscaler CEOs are in a race to create a Digital God and suggests Larry Page has said internally at Google that he's "willing to go bankrupt rather than lose this race." Companies like Google will keep spending until something tells them that scaling laws are slowing, and according to them scaling laws have only slowed because NVIDIA chips haven't improved since GPT-4 was released, and so one can deduce that capex will continue to grow... We don't know if transformers will scale to the point of AGI, but in the minds of large companies, they better because that's the only way any of this will financially make sense. Sam was not being tongue-in-cheek when he [said](https://www.youtube.com/watch?v=TzcJlKg2Rc0&t=1886s) that OpenAI's business model was to create AGI and then ask it how to return investor capital.

The premature rush to the AI endgame poses significant risks. We appear to be massively over-indexing on an approach that shows diminishing returns, potentially at the cost of exploring more innovative and efficient paths to advanced reasoning. I am not saying that AI on its current trajectory isn't insanely valuable in its own regard, nor am I saying that the next lift provided by new compute will accelerate progress. I will even concede that if transformers were to not be the architecture of the future, then this isn't all for naught as data centers can be repurposed. But this next shift will be similar to those of yesteryear. We may be over our skis. This is not the way we'll usher in AGI. 

### A New Hope

Transformers excel at recognizing and reproducing patterns they've memorized from their training data but struggle with tasks that require genuine reasoning. More training data does lead to better results on memorization-based benchmarks but offers only the illusion of general intelligence. Given the black box, it's not clear whether more data actually improves reasoning, or if more data simply offers more potential patterns to be uncovered across contexts. Some well-funded companies are developing their own reasoning models, but their methods are reliant on [gambits](https://www.theinformation.com/articles/how-openais-smaller-rivals-are-developing-their-own-ai-that-reasons?rc=ir1qfp) in of themselves.

François Chollet offered a new [benchmark](https://arxiv.org/abs/1911.01547) in 2019 (formalized via a [competition](https://arcprize.org/) this summer) that shot to the forefront of the AI zeitgeist called Abstraction and Reasoning Corpus for Artificial General Intelligence, or ARC-AGI. In the prize [announcement](https://arcprize.org/blog/launch), the team states that greater scale will not enable LLMs to learn new skills. Instead, new generalizable architectures are needed to adapt to novel situations. ARC is designed to test an AI system's ability to understand abstract patterns and apply that understanding to novel situations while being explicitly resistant to memorization, the core strength of transformers. The SOTA [score](https://www.kaggle.com/competitions/arc-prize-2024/leaderboard) to date on ARC is 46% from the MindsAI team led by [Jack Cole](https://x.com/mindsai_jack). An 85% is needed to claim the $500K grand prize.

While we don't know the methods of those atop the current leaderboard, I want to propose the intriguing potential solution that is  neurosymbolic AI (NSAI). This approach combines the best parts of self-supervised deep learning - pattern recognition and large-scale data retrieval - with symbolic approaches - explicit knowledge representation and logical reasoning - each making up for the other's [shortcomings](https://www.youtube.com/watch?v=eHWZYURQvGw&t=957s).

A great comparison is that of Daniel Kahneman's ["Thinking Fast and Slow"](https://kahneman.scholar.princeton.edu/publications) where neural nets represent system 1 thinking and symbolic AI system 2. From [DeepMind](https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/):

> Because language models excel at identifying general patterns and relationships in data, they can quickly predict potentially useful constructs, but often lack the ability to reason rigorously or explain their decisions. Symbolic deduction engines, on the other hand, are based on formal logic and use clear rules to arrive at conclusions. They are rational and explainable, but they can be “slow” and inflexible - especially when dealing with large, complex problems on their own.

Symbolic and sub-symbolic systems (like transformers) are [complementary](https://www.semantic-web-journal.net/system/files/swj2291.pdf) to one another. While transformers are robust against noise and excel at pattern recognition, symbolic systems thrive at tasks involving structured data and explicit reasoning. The challenge is reconciling the fundamentally different ways these systems represent information.

NSAI attempts to address this challenge by creating an integration that can leverage the strengths of both systems. The integration of these can be broadly categorized across two main components:

1. A neural component, based on deep learning architectures, which can process raw, unstructured data and do pattern recognition.
2. A symbolic component that can handle knowledge representation, logical reasoning, and the manipulation of abstract concepts.

The actual integration can then be achieved through [two approaches](https://arxiv.org/abs/2305.00813):

1. Knowledge compression for neural integration which involves techniques like knowledge graph embedding and logic-based compression.
2. Neural pattern lifting for symbolic integration which includes methods like decoupled and intertwined integration.

![New Hope](/images/new-hope.png)
[*Neurosymbolic AI - Why, What, and How (Sheth, Roy, & Gaur)*](https://arxiv.org/abs/2305.00813)

When compared to mainstream LLM architectures, NSAI offers manifold advantages. First is its enhanced reasoning and generalization capabilities. By incorporating oft-dismissed symbolic reasoning, these systems can generalize from fewer examples and apply knowledge more flexibly to new situations, exactly the kind of things ARC is testing for and what transformers struggle with.

As a result, NSAI systems may require less training data to achieve higher performance, addressing one of the key limitations of transformers. This could democratize development, making it accessible to domains where large datasets are scarce or impossible to obtain. (Quick tangent: check out [DisTrO](https://github.com/NousResearch/DisTrO/blob/main/A_Preliminary_Report_on_DisTrO.pdf) from Nous Research for a snapshot of more efficient LLM training).

NSAI also provides improved interpretability and explainability. Unlike the black boxes of transformers, these outputs are more transparent, offering a sort of audit trail for each step in the process. This transparency is critical for applications in nuanced fields where understanding the "why" is just as important as the decision itself.

Perhaps most importantly though is that NSAI better aligns with human cognition. The combination of neural and symbolic processing mirrors how humans are believed to think and reason (back to Kahneman's system 1 versus system 2). MIT's [Computational Cognitive Science](https://cocosci.mit.edu/) lab led by [Josh Tenenbaum](https://cocosci.mit.edu/josh) offers a wealth of knowledge and deeper research on the subject. (Quick tangent: this [thesis](https://samacquaviva.com/projects/assets/thesis.pdf) from Sam Acquaviva is a great resource on program induction).

Looking forward, the potential applications of NSAI are vast. From actually making personal agents a thing to advanced scientific discovery tools, the integration of neural and symbolic approaches promises to make up for the lost time and money (again, for some, not all) caused by transformers. However, realizing this potential will require sustained research effort and funding.

Luckily, we're finally beginning to see NSAI approaches hit the mainstream. In January, DeepMind released [AlphaGeometry](https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/), capable of solving complex geometry problems at an International Mathematical Olympic Gold Medalist level. Its language model guides a symbolic deduction method towards the possible range of solutions. More recently, a company called [Symbolica](https://www.symbolica.ai/) [launched](https://venturebeat.com/ai/move-over-deep-learning-symbolicas-structured-approach-could-transform-ai/) with $33M in funding. Symbolica is attempting to create a reasoning model via an approach rooted in [category theory](https://arxiv.org/abs/2402.15332), which relates mathematical structures to one another. If you know of anybody else making the emergence and adoption of NSAI their mission, please reach out.

---

We've reached a juncture in the AI wave and it's clear that the endgame has been called prematurely. Pieces have been sacrificed for perceived advantages as the industry has gone all in on transformers and continues to blitzscale to beat out the competition.

The transformer architecture (for all its impressive capabilities and economic importance) has fundamental flaws. The quadratic complexity problem has led to a series of incremental gambits that ultimately fail to solve the unsolvable. The reproduction and interpolation they're so good at do not yield the abstract generalization that is human-level intelligence. The spending in pursuit of the impossibly distant AGI has swelled to concerning levels. The disconnect between both the numbers and the commentary raises serious questions about the long-term viability of this strategy.

Neurosymbolic AI does offer hope. By combining the pattern recognition strengths of neural networks with the explicit reasoning capabilities of symbolic systems, NSAI promises a more balanced and potentially fruitful approach to AGI. Today it's still a pawn, but as it marches down the board, it will hopefully soon become a queen to aid our beleaguered king.

In chess, the most successful players are those who can adapt their strategy as the game evolves, and who can see beyond the immediate tactics to the larger strategic picture. The same is true in AI research and AI itself. We must be willing to reconsider our opening moves, develop new strategies for the middlegame, and redefine what victory looks like in the endgame.